{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils import *\n",
    "\n",
    "import itertools\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "from imblearn.pipeline import Pipeline as Imb_Pipeline\n",
    "\n",
    "import joblib\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.impute import MissingIndicator\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, make_scorer\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "import tensorflow.keras.regularizers as regularizers\n",
    "\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global settings\n",
    "protocol = 'thrombusresolution'\n",
    "n_jobs = -1\n",
    "k_fold = 5\n",
    "n_repeats = 40\n",
    "n_iter = 1000\n",
    "seed = 12345\n",
    "np.random.seed(seed)\n",
    "verbose = False\n",
    "return_train_score =  False\n",
    "summary_dict = {}\n",
    "key_dict = {0:'Resolved',1:'Unresolved'}\n",
    "epochs = 30\n",
    "bootstrap_reps = 1000\n",
    "missing_indicator = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome = pd.read_csv('processed_data\\\\outcome.csv')\n",
    "predictors = pd.read_csv('processed_data\\\\predictors.csv')\n",
    "categorical_features = pd.read_csv('processed_data\\\\categorical_features.csv').values.tolist()\n",
    "categorical_features = [item for sublist in categorical_features for item in sublist]\n",
    "numeric_features = pd.read_csv('processed_data\\\\numeric_features.csv').values.tolist()\n",
    "numeric_features = [item for sublist in numeric_features for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 244\n",
      "\n",
      "Class Breakdown, count:\n",
      "0    156\n",
      "1     88\n",
      "Name: lvtstatus, dtype: int64\n",
      "\n",
      "Class Breakdown, %:\n",
      "0    0.639344\n",
      "1    0.360656\n",
      "Name: lvtstatus, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(f'Dataset size: {len(outcome)}')\n",
    "print()\n",
    "print('Class Breakdown, count:')\n",
    "print(outcome['lvtstatus'].value_counts())\n",
    "print()\n",
    "print('Class Breakdown, %:')\n",
    "print(outcome['lvtstatus'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,_,_,_ = train_test_split(predictors,outcome,test_size=0.3,random_state=seed,stratify=outcome)\n",
    "train_indices = x_train.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All predictors:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Age, years',\n",
       " 'Sex',\n",
       " 'Height, cm',\n",
       " 'Weight, kg',\n",
       " 'Ethnicity',\n",
       " 'Smoking',\n",
       " 'Alcohol Use',\n",
       " 'Atrial Fibrillation',\n",
       " 'Existing Anticoagulation Before Diagnosis Of LV Thrombus',\n",
       " 'Hypertension',\n",
       " 'Hyperlipidemia',\n",
       " 'Diabetes Mellitus',\n",
       " 'Chronic Kidney Disease',\n",
       " 'Chronic Liver Disease',\n",
       " 'Peripheral Vascular Disease',\n",
       " 'Venous Thromboembolism',\n",
       " 'Cerebrovascular Accident',\n",
       " 'Asthma',\n",
       " 'Chronic Obstructive Pulmonary Disease',\n",
       " 'Malignancy',\n",
       " 'Prior Acute Coronary Syndrome',\n",
       " 'Heart Failure',\n",
       " 'Post-AMI Atrial Fibrillation',\n",
       " 'Post-AMI Cardiogenic Shock',\n",
       " 'Cardiopulmonary Resuscitation',\n",
       " 'Peak Troponin I, ng/dL',\n",
       " 'Hemoglobin, g/dL',\n",
       " 'White Blood Cell Count, 10^9/L',\n",
       " 'Lymphocyte Count, 10^9/L',\n",
       " 'Neutrophil Count, 10^9/L',\n",
       " 'Platelet Count, 10^9/dL',\n",
       " 'Prothrombin Time, seconds',\n",
       " 'International Normalized Ratio',\n",
       " 'Activated Partial Thromboplastin Time',\n",
       " 'Aspartate Aminotransferase, U/L',\n",
       " 'Alanine Aminotransferase, U/L',\n",
       " 'Alkaline Phosphatase, U/L',\n",
       " 'Creatinine, mmol/L',\n",
       " 'ACS Type',\n",
       " 'Visual Ejection Fraction, %',\n",
       " 'LVIDd, mm',\n",
       " 'LVIDs, mm',\n",
       " 'LVOT, mm',\n",
       " 'Wall Motion Abnormality',\n",
       " 'Left Ventricular Aneurysm',\n",
       " 'LV Thrombus Mobility',\n",
       " 'Protrusion',\n",
       " 'LV Thrombus Maximal Diameter, cm',\n",
       " 'Aspirin Use',\n",
       " 'Second Antiplatelet Agent',\n",
       " 'Coronary Artery Disease',\n",
       " 'Revascularization Procedure',\n",
       " 'Type Of Stent Used']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('All predictors:')\n",
    "list(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_and_encode(df,train_indices,categorical_features=categorical_features):\n",
    "    \"\"\"\n",
    "    Takes a dataframe and perform univariate imputation by column\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas.DataFrame\n",
    "        Dataset to be imputed.\n",
    "    train_indices: array-like\n",
    "        An array of indices for training data - used to fit SimpleImputer obtain\n",
    "    categorical_features: list\n",
    "        An list of strings containing column names for categorical objects. Used to determine type of imputation and whether centering and scaling is necessary\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    imputed_df: pandas.DataFrame\n",
    "        A dataframe containing the imputed and scaled dataset\n",
    "        \n",
    "    \"\"\"\n",
    "    imputed_df = pd.DataFrame()\n",
    "    for column in df.columns:\n",
    "        if df[column].isna().sum() != 0:\n",
    "            array = df[column].values.reshape(-1, 1)\n",
    "            if df[column].dtype == 'float64': \n",
    "                si = SimpleImputer(strategy='mean',missing_values=np.nan,add_indicator=missing_indicator)\n",
    "                si.fit(array[train_indices])\n",
    "                out = si.transform(array)\n",
    "            else:\n",
    "                si = SimpleImputer(strategy='most_frequent',missing_values=np.nan,add_indicator=missing_indicator)\n",
    "                si.fit(array[train_indices])\n",
    "                out = si.transform(array)\n",
    "            if out.shape[1] == 1:\n",
    "                out = out.flatten()\n",
    "                imputed_df[column] = out\n",
    "            else:\n",
    "                imputed_df[column] = out[:,0]\n",
    "                imputed_df[column+'_missing'] = out[:,1].astype('bool') \n",
    "        else:\n",
    "            imputed_df[column] = df[column]\n",
    "    \n",
    "    for column in df.columns:\n",
    "        if column not in categorical_features:\n",
    "            array = imputed_df[column].values.reshape(-1, 1)\n",
    "            std_scaler = StandardScaler()\n",
    "            std_scaler.fit(array[train_indices])\n",
    "            out = std_scaler.transform(array)\n",
    "            out = out.flatten()\n",
    "            imputed_df[column] = out\n",
    "    \n",
    "    for varname in categorical_features:\n",
    "        onehot = pd.get_dummies(imputed_df[varname],prefix=varname,prefix_sep='_',drop_first=False)\n",
    "        imputed_df = imputed_df.drop(varname,axis=1).join(onehot)\n",
    "    return imputed_df\n",
    "predictors = impute_and_encode(predictors,train_indices=train_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age, years</th>\n",
       "      <th>Height, cm</th>\n",
       "      <th>Weight, kg</th>\n",
       "      <th>Peak Troponin I, ng/dL</th>\n",
       "      <th>Hemoglobin, g/dL</th>\n",
       "      <th>White Blood Cell Count, 10^9/L</th>\n",
       "      <th>Lymphocyte Count, 10^9/L</th>\n",
       "      <th>Neutrophil Count, 10^9/L</th>\n",
       "      <th>Platelet Count, 10^9/dL</th>\n",
       "      <th>Prothrombin Time, seconds</th>\n",
       "      <th>...</th>\n",
       "      <th>Coronary Artery Disease_Single Vessel Disease</th>\n",
       "      <th>Coronary Artery Disease_Triple Vessel Disease</th>\n",
       "      <th>Revascularization Procedure_Coronary Artery Bypass Graft</th>\n",
       "      <th>Revascularization Procedure_No Revascularization</th>\n",
       "      <th>Revascularization Procedure_Percutaneous Coronary Intervention</th>\n",
       "      <th>Revascularization Procedure_Thrombolysis</th>\n",
       "      <th>Type Of Stent Used_Bare Metal Stent</th>\n",
       "      <th>Type Of Stent Used_Bioabsorbable Vascular Stent</th>\n",
       "      <th>Type Of Stent Used_Drug-eluting Stent</th>\n",
       "      <th>Type Of Stent Used_POBA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.283460</td>\n",
       "      <td>-1.536079e-01</td>\n",
       "      <td>-0.522832</td>\n",
       "      <td>-1.068690</td>\n",
       "      <td>-1.757637</td>\n",
       "      <td>-0.978320</td>\n",
       "      <td>1.962009e+00</td>\n",
       "      <td>-1.810099e+00</td>\n",
       "      <td>-0.439196</td>\n",
       "      <td>-1.151485</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.205862</td>\n",
       "      <td>1.241593e+00</td>\n",
       "      <td>1.032187</td>\n",
       "      <td>-0.920256</td>\n",
       "      <td>0.932812</td>\n",
       "      <td>-0.892661</td>\n",
       "      <td>7.786424e-01</td>\n",
       "      <td>-1.359915e+00</td>\n",
       "      <td>0.133043</td>\n",
       "      <td>-1.320228</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.337322</td>\n",
       "      <td>1.357860e+00</td>\n",
       "      <td>1.771769</td>\n",
       "      <td>-1.139710</td>\n",
       "      <td>-0.742373</td>\n",
       "      <td>0.707935</td>\n",
       "      <td>2.282393e-01</td>\n",
       "      <td>4.408216e-01</td>\n",
       "      <td>2.251545</td>\n",
       "      <td>-1.404599</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.749047</td>\n",
       "      <td>-2.698746e-01</td>\n",
       "      <td>-1.768112</td>\n",
       "      <td>-1.227710</td>\n",
       "      <td>-1.706874</td>\n",
       "      <td>0.497459</td>\n",
       "      <td>-1.670651e+00</td>\n",
       "      <td>-4.939557e-01</td>\n",
       "      <td>2.458525</td>\n",
       "      <td>-0.645256</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.026931</td>\n",
       "      <td>1.474126e+00</td>\n",
       "      <td>0.191465</td>\n",
       "      <td>-1.227399</td>\n",
       "      <td>-0.488557</td>\n",
       "      <td>-1.489825</td>\n",
       "      <td>7.229175e-02</td>\n",
       "      <td>-1.643789e+00</td>\n",
       "      <td>-0.877507</td>\n",
       "      <td>-0.603070</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>-0.958105</td>\n",
       "      <td>-2.581640e-15</td>\n",
       "      <td>-1.218166</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.729759</td>\n",
       "      <td>2.122528</td>\n",
       "      <td>4.073801e-16</td>\n",
       "      <td>5.093552e-16</td>\n",
       "      <td>-0.950559</td>\n",
       "      <td>0.240644</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>-2.199670</td>\n",
       "      <td>4.277258e-01</td>\n",
       "      <td>0.450635</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.120601</td>\n",
       "      <td>-0.048310</td>\n",
       "      <td>4.073801e-16</td>\n",
       "      <td>5.093552e-16</td>\n",
       "      <td>1.423625</td>\n",
       "      <td>-0.476513</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>-1.578888</td>\n",
       "      <td>1.951923e-01</td>\n",
       "      <td>-0.503869</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.073801e-16</td>\n",
       "      <td>5.093552e-16</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>-0.647714</td>\n",
       "      <td>-1.316275e+00</td>\n",
       "      <td>1.082757</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.526707</td>\n",
       "      <td>0.228245</td>\n",
       "      <td>4.073801e-16</td>\n",
       "      <td>5.093552e-16</td>\n",
       "      <td>0.023465</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>-0.259725</td>\n",
       "      <td>-2.581640e-15</td>\n",
       "      <td>-1.028530</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.678996</td>\n",
       "      <td>1.094623</td>\n",
       "      <td>4.073801e-16</td>\n",
       "      <td>5.093552e-16</td>\n",
       "      <td>0.072166</td>\n",
       "      <td>2.054631</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>244 rows Ã— 105 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Age, years    Height, cm  Weight, kg  Peak Troponin I, ng/dL  \\\n",
       "0      0.283460 -1.536079e-01   -0.522832               -1.068690   \n",
       "1      0.205862  1.241593e+00    1.032187               -0.920256   \n",
       "2     -0.337322  1.357860e+00    1.771769               -1.139710   \n",
       "3      0.749047 -2.698746e-01   -1.768112               -1.227710   \n",
       "4     -0.026931  1.474126e+00    0.191465               -1.227399   \n",
       "..          ...           ...         ...                     ...   \n",
       "239   -0.958105 -2.581640e-15   -1.218166                0.000000   \n",
       "240   -2.199670  4.277258e-01    0.450635                0.000000   \n",
       "241   -1.578888  1.951923e-01   -0.503869                0.000000   \n",
       "242   -0.647714 -1.316275e+00    1.082757                0.000000   \n",
       "243   -0.259725 -2.581640e-15   -1.028530                0.000000   \n",
       "\n",
       "     Hemoglobin, g/dL  White Blood Cell Count, 10^9/L  \\\n",
       "0           -1.757637                       -0.978320   \n",
       "1            0.932812                       -0.892661   \n",
       "2           -0.742373                        0.707935   \n",
       "3           -1.706874                        0.497459   \n",
       "4           -0.488557                       -1.489825   \n",
       "..                ...                             ...   \n",
       "239          0.729759                        2.122528   \n",
       "240          0.120601                       -0.048310   \n",
       "241          0.000000                        0.000000   \n",
       "242          0.526707                        0.228245   \n",
       "243          0.678996                        1.094623   \n",
       "\n",
       "     Lymphocyte Count, 10^9/L  Neutrophil Count, 10^9/L  \\\n",
       "0                1.962009e+00             -1.810099e+00   \n",
       "1                7.786424e-01             -1.359915e+00   \n",
       "2                2.282393e-01              4.408216e-01   \n",
       "3               -1.670651e+00             -4.939557e-01   \n",
       "4                7.229175e-02             -1.643789e+00   \n",
       "..                        ...                       ...   \n",
       "239              4.073801e-16              5.093552e-16   \n",
       "240              4.073801e-16              5.093552e-16   \n",
       "241              4.073801e-16              5.093552e-16   \n",
       "242              4.073801e-16              5.093552e-16   \n",
       "243              4.073801e-16              5.093552e-16   \n",
       "\n",
       "     Platelet Count, 10^9/dL  Prothrombin Time, seconds  ...  \\\n",
       "0                  -0.439196                  -1.151485  ...   \n",
       "1                   0.133043                  -1.320228  ...   \n",
       "2                   2.251545                  -1.404599  ...   \n",
       "3                   2.458525                  -0.645256  ...   \n",
       "4                  -0.877507                  -0.603070  ...   \n",
       "..                       ...                        ...  ...   \n",
       "239                -0.950559                   0.240644  ...   \n",
       "240                 1.423625                  -0.476513  ...   \n",
       "241                 0.000000                   0.000000  ...   \n",
       "242                 0.023465                   0.000000  ...   \n",
       "243                 0.072166                   2.054631  ...   \n",
       "\n",
       "     Coronary Artery Disease_Single Vessel Disease  \\\n",
       "0                                                0   \n",
       "1                                                0   \n",
       "2                                                0   \n",
       "3                                                0   \n",
       "4                                                1   \n",
       "..                                             ...   \n",
       "239                                              1   \n",
       "240                                              0   \n",
       "241                                              0   \n",
       "242                                              1   \n",
       "243                                              0   \n",
       "\n",
       "     Coronary Artery Disease_Triple Vessel Disease  \\\n",
       "0                                                1   \n",
       "1                                                0   \n",
       "2                                                0   \n",
       "3                                                1   \n",
       "4                                                0   \n",
       "..                                             ...   \n",
       "239                                              0   \n",
       "240                                              0   \n",
       "241                                              0   \n",
       "242                                              0   \n",
       "243                                              1   \n",
       "\n",
       "     Revascularization Procedure_Coronary Artery Bypass Graft  \\\n",
       "0                                                    0          \n",
       "1                                                    0          \n",
       "2                                                    0          \n",
       "3                                                    0          \n",
       "4                                                    0          \n",
       "..                                                 ...          \n",
       "239                                                  0          \n",
       "240                                                  0          \n",
       "241                                                  0          \n",
       "242                                                  0          \n",
       "243                                                  0          \n",
       "\n",
       "     Revascularization Procedure_No Revascularization  \\\n",
       "0                                                   0   \n",
       "1                                                   0   \n",
       "2                                                   0   \n",
       "3                                                   1   \n",
       "4                                                   0   \n",
       "..                                                ...   \n",
       "239                                                 0   \n",
       "240                                                 0   \n",
       "241                                                 0   \n",
       "242                                                 0   \n",
       "243                                                 1   \n",
       "\n",
       "     Revascularization Procedure_Percutaneous Coronary Intervention  \\\n",
       "0                                                    1                \n",
       "1                                                    1                \n",
       "2                                                    1                \n",
       "3                                                    0                \n",
       "4                                                    1                \n",
       "..                                                 ...                \n",
       "239                                                  1                \n",
       "240                                                  1                \n",
       "241                                                  1                \n",
       "242                                                  1                \n",
       "243                                                  0                \n",
       "\n",
       "     Revascularization Procedure_Thrombolysis  \\\n",
       "0                                           0   \n",
       "1                                           0   \n",
       "2                                           0   \n",
       "3                                           0   \n",
       "4                                           0   \n",
       "..                                        ...   \n",
       "239                                         0   \n",
       "240                                         0   \n",
       "241                                         0   \n",
       "242                                         0   \n",
       "243                                         0   \n",
       "\n",
       "     Type Of Stent Used_Bare Metal Stent  \\\n",
       "0                                      0   \n",
       "1                                      0   \n",
       "2                                      0   \n",
       "3                                      0   \n",
       "4                                      0   \n",
       "..                                   ...   \n",
       "239                                    1   \n",
       "240                                    1   \n",
       "241                                    1   \n",
       "242                                    1   \n",
       "243                                    0   \n",
       "\n",
       "     Type Of Stent Used_Bioabsorbable Vascular Stent  \\\n",
       "0                                                  0   \n",
       "1                                                  0   \n",
       "2                                                  0   \n",
       "3                                                  0   \n",
       "4                                                  0   \n",
       "..                                               ...   \n",
       "239                                                0   \n",
       "240                                                0   \n",
       "241                                                0   \n",
       "242                                                0   \n",
       "243                                                0   \n",
       "\n",
       "     Type Of Stent Used_Drug-eluting Stent  Type Of Stent Used_POBA  \n",
       "0                                        1                        0  \n",
       "1                                        1                        0  \n",
       "2                                        1                        0  \n",
       "3                                        1                        0  \n",
       "4                                        1                        0  \n",
       "..                                     ...                      ...  \n",
       "239                                      0                        0  \n",
       "240                                      0                        0  \n",
       "241                                      0                        0  \n",
       "242                                      0                        0  \n",
       "243                                      1                        0  \n",
       "\n",
       "[244 rows x 105 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(predictors,outcome,test_size=0.25,random_state=seed,stratify=outcome)\n",
    "y_train = y_train.values.flatten()\n",
    "y_test = y_test.values.flatten()\n",
    "batch_size = len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(x_train).to_csv('processed_data\\\\x_train.csv')\n",
    "pd.DataFrame(x_test).to_csv('processed_data\\\\x_test.csv')\n",
    "pd.DataFrame(y_train).to_csv('processed_data\\\\y_train.csv')\n",
    "pd.DataFrame(y_test).to_csv('processed_data\\\\y_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_fn(n_layers=1,deep_units=128,deep_dropout_prob=0.2,preoutput_layer=64,dropout_rate=0.2,l1_reg=0.01,l2_reg=0.01):\n",
    "    \"\"\"\n",
    "    Function to build tf.keras model\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_layers: int\n",
    "        Number of hidden layers before 2nd last layer\n",
    "    deep_units: int\n",
    "        Number of units in hidden layers\n",
    "    deep_dropout_prob: float64 (0.0-1.0)\n",
    "        Probability of having a dropout layer between each deep layer\n",
    "    preoutput_layer: int\n",
    "        Number of units in penultimate layer\n",
    "    dropout_rate: float64\n",
    "        Dropout rate for dropout layers\n",
    "    l1_reg: float64\n",
    "        L1 regularizer for deep layers\n",
    "    l2_reg: float64\n",
    "        L2 regularizer for deep layers\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    model: tf.keras model\n",
    "        The built model object\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    for layer in range(n_layers):\n",
    "        model.add(Dense(deep_units,activation='relu',kernel_regularizer=regularizers.l2(l2_reg),activity_regularizer=regularizers.l1(l1_reg)))\n",
    "        if np.random.random() > deep_dropout_prob:\n",
    "            model.add(Dropout(rate=dropout_rate))\n",
    "    model.add(Dense(preoutput_layer,activation='relu',kernel_regularizer=regularizers.l2(l2_reg),activity_regularizer=regularizers.l1(l1_reg)))\n",
    "    model.add(Dropout(rate=dropout_rate))\n",
    "    model.add(Dense(1,activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_selection(summary_dict,model_lst,param_dict,technique,x_train=x_train,y_train=y_train,x_test=x_test,y_test=y_test,n_iter=n_iter,k_fold=k_fold,n_repeats=n_repeats):\n",
    "    \"\"\"\n",
    "    A wrapper function for the model selection loop\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    summary_dict: dict\n",
    "        An empty dictionary used to store results.\n",
    "    model_lst: list\n",
    "        A list of tuples containing ('model_name',model), models are sklearn estimators\n",
    "    param_dict: dict\n",
    "        A dictionary containing model parameter distributions - to be passed to RandomizedSearchCV\n",
    "    technique: str\n",
    "        A string indicating technique used. Only relevant if testing techniques such as oversampling/SMOTE.\n",
    "    x_train: array-like\n",
    "        An array training set predictors\n",
    "    y_train: array-like\n",
    "        An array containing training set labels\n",
    "    x_test: array-like\n",
    "        An array containing test set predictors\n",
    "    y_test: array-like\n",
    "        An array containing test set labels\n",
    "    n_iter: int\n",
    "        Number of crossvalidation iterations - to be passed to RandomizedSearchCV. Defaults to n_iter parameter at top of script\n",
    "    k_fold: int\n",
    "        Number of crossvalidation folds - to be passed to RandomizedSearchCV. Defaults to k_fold parameter at top of script\n",
    "    n_repeats: int\n",
    "        Number of crossvalidation repeats - to be passed to RandomizedSearchCV. Defaults to n_repeats parameter at top of script\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    summary_dict: pandas.DataFrame\n",
    "        A dataframe containing the best model object and associated crossvalidation results\n",
    "    result_table: pandas.DataFrame\n",
    "        A dataframe containing all model objects and associated crossvalidation results\n",
    "    \"\"\"\n",
    "    iterations = n_iter\n",
    "    scoring = {'roc_auc':'roc_auc','average_precision':'average_precision','accuracy': 'accuracy'}\n",
    "    \n",
    "    result_list = []\n",
    "    for name, model in model_lst:\n",
    "\n",
    "        cv = RepeatedStratifiedKFold(n_splits=k_fold,n_repeats=n_repeats)\n",
    "        refit_score = 'roc_auc'\n",
    "        \n",
    "        estimator = model\n",
    "        if name =='DNN':\n",
    "            n_jobs = 1\n",
    "            x_train = x_train.to_numpy().astype('float64')\n",
    "            if iterations > 10:\n",
    "                iterations = np.amin([iterations,100])\n",
    "        else:\n",
    "            n_jobs = -1\n",
    "\n",
    "        search = RandomizedSearchCV(estimator,param_distributions=param_dict.get(name),random_state=seed,cv=cv,n_iter=iterations,n_jobs=n_jobs,\n",
    "                                      scoring=scoring,refit=refit_score,verbose=verbose,return_train_score=return_train_score)\n",
    "        search.fit(x_train, y_train)\n",
    "\n",
    "        print(f'Model: {name}')\n",
    "        if search.best_score_ == 0:\n",
    "            print('No candidate models met minimum requirements')\n",
    "        else:\n",
    "            y_pred = search.best_estimator_.predict(x_train)\n",
    "            if name == 'DNN':\n",
    "                y_pred = y_pred > 0.5\n",
    "\n",
    "            print('Classification report of best model:')\n",
    "            print(classification_report(y_true=y_train,y_pred=y_pred))\n",
    "            print(f'CV score of best model: {search.best_score_}')\n",
    "\n",
    "        result_list.append((name,search,search.best_score_,search.cv_results_))\n",
    "        print()\n",
    "    \n",
    "    print(f'Model: Ensemble')\n",
    "    unpacked_results = [(r[0],r[1].best_estimator_) for r in result_list if r[0] not in ['svm','DNN']]\n",
    "    ensemble = VotingClassifier(unpacked_results,voting='soft')\n",
    "    cross_v = cross_validate(ensemble,x_train,y_train,cv=cv,n_jobs=n_jobs,scoring=scoring)\n",
    "    ensemble.fit(x_train,y_train)\n",
    "    y_pred = ensemble.predict(x_train)\n",
    "    print(classification_report(y_true=y_train,y_pred=y_pred))\n",
    "    score_dict = {}\n",
    "    for i in range(k_fold*n_repeats):\n",
    "        score_dict[f'split{i}_test_roc_auc'] = cross_v['test_roc_auc']\n",
    "        score_dict[f'split{i}_test_average_precision'] = cross_v['test_average_precision']\n",
    "        score_dict[f'split{i}_test_accuracy'] = cross_v['test_accuracy']\n",
    "    mean_cv_roc_auc = np.mean(cross_v['test_roc_auc'])\n",
    "    print(f'Cross-Validation Score:{mean_cv_roc_auc}')\n",
    "    result_list.append(('Ensemble',ensemble,mean_cv_roc_auc,score_dict))\n",
    "    print()\n",
    "    \n",
    "    result_table = pd.DataFrame(result_list,columns=['name','model','scores','score_dict'])\n",
    "    \n",
    "    best_search = result_table['model'][result_table['scores']==max(result_table['scores'])].values.tolist()[0]\n",
    "    print(f\"Best Cross-Validation score: {best_search.best_score_}\")\n",
    "    y_pred = best_search.best_estimator_.predict(x_train)\n",
    "    if y_pred.ndim > 1:\n",
    "        y_pred = np.argmax(y_pred,axis=-1)\n",
    "    prop = pd.Series(y_pred).value_counts(normalize=True)\n",
    "    prop = prop.rename(index=key_dict)\n",
    "    ax = prop.plot.bar(ylim=(0,1),title=\"Distribution of Predicted Labels\")\n",
    "\n",
    "    if best_search.best_score_ == 0:\n",
    "        model = None\n",
    "    else:    \n",
    "        model = result_table['name'][result_table['scores']==max(result_table['scores'])].values.tolist()[0]\n",
    "    summary_dict[technique] = {'Model':model}\n",
    "    metrics = ['mean_test_roc_auc','mean_test_average_precision','mean_test_accuracy']\n",
    "    for key in [key for key in best_search.cv_results_.keys() if key in metrics]:\n",
    "        summary_dict[technique][key.split('mean_test_')[1]] = best_search.cv_results_[key][best_search.best_index_]\n",
    "    summary_dict[technique]['model obj'] = best_search.best_estimator_\n",
    "    \n",
    "    return summary_dict, result_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: lr\n",
      "Classification report of best model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.96      0.88       117\n",
      "           1       0.89      0.61      0.72        66\n",
      "\n",
      "    accuracy                           0.83       183\n",
      "   macro avg       0.85      0.78      0.80       183\n",
      "weighted avg       0.84      0.83      0.82       183\n",
      "\n",
      "CV score of best model: 0.7577950310559006\n",
      "\n",
      "Model: svm\n",
      "Classification report of best model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.79      0.83       117\n",
      "           1       0.68      0.82      0.74        66\n",
      "\n",
      "    accuracy                           0.80       183\n",
      "   macro avg       0.78      0.80      0.79       183\n",
      "weighted avg       0.81      0.80      0.80       183\n",
      "\n",
      "CV score of best model: 0.7554912207357861\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logistic = SGDClassifier(loss='log',random_state=seed)\n",
    "svm = SGDClassifier(loss='hinge',random_state=seed)\n",
    "rf = RandomForestClassifier(random_state=seed)\n",
    "gbm = GradientBoostingClassifier(random_state=seed)\n",
    "xgbclf = xgb.XGBClassifier(objective=\"binary:logistic\",tree_method='hist',random_state=seed)\n",
    "keras_model = KerasClassifier(build_fn,epochs=epochs,batch_size=batch_size,verbose=False)\n",
    "\n",
    "classifier_list = [('lr',logistic),('svm',svm),('rf',rf),('gbm',gbm),('xgb',xgbclf)]#,('DNN',keras_model)] \n",
    "params = {'lr':{'alpha':uniform(1e-5,10),\n",
    "                'penalty':['l1', 'l2', 'elasticnet'], #l1 - lasso, #l2 - ridge, #elasticnet\n",
    "                'l1_ratio':uniform(0.01,0.30),\n",
    "                'class_weight':[None,'balanced']},\n",
    "          'svm':{'alpha':uniform(1e-5,10),\n",
    "                 'class_weight':[None,'balanced']},\n",
    "          'rf':{'bootstrap':[True],\n",
    "                'criterion':['gini','entropy'],\n",
    "                'max_depth':randint(2,10),\n",
    "                'max_features':['sqrt', 'log2'],\n",
    "                'min_samples_leaf':randint(2,20),\n",
    "                'min_samples_split':randint(2,20),\n",
    "                'n_estimators':randint(100, 2000),\n",
    "                'class_weight':[None,'balanced']},\n",
    "          'gbm':{'loss':['deviance','exponential'],\n",
    "                 'learning_rate':uniform(0.003, 0.3),\n",
    "                 'n_estimators':randint(100, 2000),\n",
    "                 'subsample':uniform(0.5, 0.5),\n",
    "                 'criterion':['friedman_mse','mse','mae'],\n",
    "                 'min_samples_split':randint(2,20),\n",
    "                 'min_samples_leaf':randint(2,20),\n",
    "                 'max_depth':randint(2,10),\n",
    "                 'max_features':['sqrt', 'log2']},\n",
    "          'xgb':{'colsample_bytree':uniform(0.7, 0.3),\n",
    "                 'gamma':uniform(0, 0.5),\n",
    "                 'learning_rate':uniform(0.003, 0.3), # default 0.1 \n",
    "                 'max_depth':randint(2,10), # default 3\n",
    "                 'n_estimators':randint(100, 2000), # default 100\n",
    "                 'subsample':uniform(0.5, 0.5),\n",
    "                 'class_weight':[None,'balanced']},\n",
    "          'DNN':{'n_layers':randint(1,100),\n",
    "                 'deep_units':randint(1,512),\n",
    "                 'preoutput_layer':randint(1,512),\n",
    "                 'dropout_rate':[np.random.uniform(low=0.0, high=0.5) for i in range(n_iter)],\n",
    "                 'deep_dropout_prob':[np.random.uniform(low=0.0, high=1.0) for i in range(n_iter)],\n",
    "                 'l1_reg':[np.random.uniform(low=0.0, high=0.1) for i in range(n_iter)],\n",
    "                 'l2_reg':[np.random.uniform(low=0.0, high=0.1) for i in range(n_iter)]}}\n",
    "\n",
    "summary_dict, conventional_results = model_selection(summary_dict=summary_dict,model_lst=classifier_list,param_dict=params,technique='conventional')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = pd.DataFrame.from_dict(summary_dict,orient='index').applymap(lambda cell: np.round(cell,2) if isinstance(cell,float) else cell)\n",
    "summary.to_csv(f'results.csv')\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This loop extracts the 95% confidence intervals for performance metrics obtained during repeated k-fold crossvalidation\n",
    "confidence_intervals = []\n",
    "for index in range(len(conventional_results)):\n",
    "    cv_dict = {}\n",
    "    name = conventional_results['name']\n",
    "    roc_list = []\n",
    "    for i in range(k_fold*n_repeats):\n",
    "        roc_list.append(conventional_results['score_dict'][index].get(f'split{i}_test_roc_auc'))\n",
    "    array = np.array(roc_list)\n",
    "    array = array.flatten()\n",
    "    lowerbound = np.quantile(array,0.025)\n",
    "    upperbound = np.quantile(array,0.975)\n",
    "    confidence_intervals.append((lowerbound,upperbound))\n",
    "confidence_intervals = pd.DataFrame(confidence_intervals,columns=['0.025%','0.975%'])\n",
    "conventional_results = conventional_results.join(confidence_intervals)\n",
    "conventional_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    best_model = summary['model obj'][summary['roc_auc'] == max(summary['roc_auc'])][0]\n",
    "    best_technique = summary.index[summary['roc_auc'] == max(summary['roc_auc'])][0]\n",
    "    print(f'Model: {best_model}')\n",
    "    print()\n",
    "    y_pred = best_model.predict(x_test)\n",
    "    print('Classification report on test set:')\n",
    "    print(classification_report(y_true=y_test,y_pred=y_pred))\n",
    "    joblib.dump(best_model,f'pickled_objects\\\\{protocol}_best_model.pkl')\n",
    "    conventional_results.to_csv('train_results.csv')\n",
    "except BaseException as e:\n",
    "    print('No model found')\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_statistics(clf,x_test,y_test,bootstrap_reps):\n",
    "    \"\"\"\n",
    "    Nonparametric bootstrap to obtain confidence intervals for model performance on the test set.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    clf: sklearn.Estimator\n",
    "        A fitted sklearn model\n",
    "    x_test: array-like\n",
    "        Array of test set predictors\n",
    "    y_test: array-like\n",
    "        Array of test set labels\n",
    "    bootstrap_reps: int\n",
    "        Number of bootstrap replicates\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    output_dict: dict\n",
    "        A nested dictionary containing the bootstrap results for the following summary statistics: AUROC, AUPRC, Sensitivity, Specificity, PPV\n",
    "        Also contains a nested list of indices for each bootstrap value \n",
    "    \n",
    "    \"\"\"\n",
    "    bs_auprc = []\n",
    "    bs_auc = []\n",
    "    bs_sensitivity = []\n",
    "    bs_specificity = []\n",
    "    bs_ppv = []\n",
    "    index_list = []\n",
    "    for i in range(bootstrap_reps):\n",
    "        idx = np.random.choice(np.array(range(len(x_test))),size=len(x_test),replace=True)\n",
    "        y_pred_proba = clf.predict_proba(x_test.iloc[idx])[::,1]\n",
    "        ap_score = average_precision_score(y_test[idx], y_pred_proba)\n",
    "        bs_auprc.append(ap_score)\n",
    "        auc_score = roc_auc_score(y_test[idx], y_pred_proba)\n",
    "        bs_auc.append(auc_score)\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test[idx], y_pred_proba >= 0.5).ravel()\n",
    "        sensitivity = tp/(tp+fn) \n",
    "        specificity = tn/(tn+fp)\n",
    "        ppv = tp/(tp+fp)\n",
    "        bs_sensitivity.append(sensitivity)\n",
    "        bs_specificity.append(specificity)\n",
    "        bs_ppv.append(ppv)\n",
    "        index_list.append(idx.tolist())\n",
    "    \n",
    "    output_dict = {'auprc':bs_auprc,'auc':bs_auc,'sensitivity':bs_sensitivity,'specificity':bs_specificity,'ppv':bs_ppv,'indices':index_list}\n",
    "    \n",
    "    return output_dict\n",
    "    \n",
    "bootstrap_dict = bootstrap_statistics(best_model,x_test,y_test,bootstrap_reps=bootstrap_reps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest_index(lst,value):\n",
    "    \"\"\"\n",
    "    Function to get the closest index of a value in a list\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    lst: list\n",
    "        A list where you want to find an index of a value\n",
    "    value: float64\n",
    "        A value of interest\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    index: int\n",
    "        The index of the closest value to the input value in lst\n",
    "    \"\"\"\n",
    "    try:\n",
    "        closest_value = min(lst, key=lambda x:abs(x-value))\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "    index = lst.index(closest_value)\n",
    "    return index\n",
    "\n",
    "def mean_95ci(lst,index_list):\n",
    "    \"\"\"\n",
    "    A function to obtain the mean and 95% confidence intervals from a list of boostrapped results\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    lst: list\n",
    "        A list of bootstrap results\n",
    "    index_list: lst\n",
    "        A list of indices for the test set data used during bootstrap resampling\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    mean: float64\n",
    "        Bootstrap mean for the statistic of interest\n",
    "    lowerbound: float64\n",
    "        Bootstrap 2.5% quantile for the statistic of interest\n",
    "    upperbound: float64\n",
    "        Bootstrap 97.% quantile for the statistic of interest\n",
    "    mean_indices:\n",
    "        Indices of the test set data used to obtain the bootstrap mean\n",
    "    lower_indices:\n",
    "        Indices of the test set data used to obtain the bootstrap 2.5% quantile\n",
    "    upper_indices:\n",
    "        Indices of the test set data used to obtain the bootstrap 97.5% quantile\n",
    "    \"\"\"\n",
    "    \n",
    "    array = np.array(lst)\n",
    "\n",
    "    lowerbound = np.quantile(array,0.025)\n",
    "    lower_indices = get_closest_index(lst,lowerbound)\n",
    "    lowerbound = np.round(lowerbound,3)\n",
    "\n",
    "    upperbound = np.quantile(array,0.975)\n",
    "    upper_indices = get_closest_index(lst,upperbound)\n",
    "    upperbound = np.round(upperbound,3)\n",
    "\n",
    "    mean = np.mean(array)\n",
    "    mean_indices = get_closest_index(lst,mean)\n",
    "    mean = np.round(mean,3)\n",
    "\n",
    "    mean_indices = index_list[mean_indices]\n",
    "    lower_indices = index_list[lower_indices]\n",
    "    upper_indices = index_list[upper_indices]\n",
    "    \n",
    "    return mean, lowerbound, upperbound, (mean_indices,lower_indices,upper_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_auc_plot(clf,mean,lowerbound,upperbound,index_tpl):\n",
    "    \"\"\"\n",
    "    Function to plot a Receiver Operating Characteristic\n",
    "    \n",
    "    Ripped from Sklearn documentation\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    clf: sklearn.Estimator\n",
    "        A fitted sklearn model\n",
    "    mean: float64\n",
    "        Bootstrap mean of AUROC\n",
    "    lowerbound: float64\n",
    "        Bootstrap 2.5% quantile of AUROC\n",
    "    upperbound: float64\n",
    "        Bootstrap 97.5% quantile of AUROC\n",
    "    index_tpl: tuple\n",
    "        Indices of testing data for mean, lower and upper bound results\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \n",
    "    \"\"\"\n",
    "    #Get mean\n",
    "    y_pred_proba = clf.predict_proba(x_test.iloc[index_tpl[0]])[::,1]\n",
    "    mean_fpr, mean_tpr, _ = roc_curve(y_test[index_tpl[0]],  y_pred_proba)\n",
    "    \n",
    "    #Get lowerbound\n",
    "    y_pred_proba = clf.predict_proba(x_test.iloc[index_tpl[1]])[::,1]\n",
    "    lower_fpr, lower_tpr, _ = roc_curve(y_test[index_tpl[1]],  y_pred_proba)\n",
    "    \n",
    "    #Get upperbound\n",
    "    y_pred_proba = clf.predict_proba(x_test.iloc[index_tpl[2]])[::,1]\n",
    "    upper_fpr, upper_tpr, _ = roc_curve(y_test[index_tpl[2]],  y_pred_proba)\n",
    "    \n",
    "    label_string = f'AUROC: {mean}, (95% CI {lowerbound}-{upperbound})'\n",
    "    print()\n",
    "    plt.title('Receiver Operating Curve')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.plot([1, 0], [1, 0],\"k:\")\n",
    "    plt.plot(mean_fpr,mean_tpr,label=label_string,color='black')\n",
    "    #plt.plot(lower_fpr,lower_tpr,color='grey')\n",
    "    #plt.plot(upper_fpr,upper_tpr,color='grey')\n",
    "    plt.legend(loc=4)\n",
    "    plt.show()\n",
    "    \n",
    "auc_mean,auc_lowerbound,auc_upperbound,index_tpl = mean_95ci(bootstrap_dict['auc'],bootstrap_dict['indices'])\n",
    "get_auc_plot(best_model,auc_mean,auc_lowerbound,auc_upperbound,index_tpl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_auprc_plot(clf,mean,lowerbound,upperbound):\n",
    "    \n",
    "    if hasattr(clf, \"predict_proba\"):\n",
    "        y_score = clf.predict_proba(x_test)[:, 1]\n",
    "    else:  # use decision function\n",
    "        y_score = clf.decision_function(x_test)\n",
    "        y_score = (y_score - y_score.min()) / (y_score.max() - y_score.min())\n",
    "                \n",
    "    label_string = f'AUPRC: {mean}, (95% CI {lowerbound}-{upperbound})'\n",
    "    disp = plot_precision_recall_curve(clf, x_test, y_test)\n",
    "    disp.ax_.set_title(label_string)\n",
    "    \n",
    "auprc_mean,auprc_lowerbound,auprc_upperbound,_ = mean_95ci(bootstrap_dict['auprc'],bootstrap_dict['indices'])\n",
    "get_auprc_plot(best_model,auprc_mean,auprc_lowerbound,auprc_upperbound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Additional Metrics')\n",
    "sensitivity_mean,sensitivity_lowerbound,sensitivity_upperbound,_ = mean_95ci(bootstrap_dict['sensitivity'],bootstrap_dict['indices'])\n",
    "print(f'Sensitivity: {sensitivity_mean} (95% CI {sensitivity_lowerbound}-{sensitivity_upperbound})')\n",
    "specificity_mean,specificity_lowerbound,specificity_upperbound,_ = mean_95ci(bootstrap_dict['specificity'],bootstrap_dict['indices'])\n",
    "print(f'Specificity: {specificity_mean} (95% CI {specificity_lowerbound}-{specificity_upperbound})')\n",
    "ppv_mean,ppv_lowerbound,ppv_upperbound,_ = mean_95ci(bootstrap_dict['ppv'],bootstrap_dict['indices'])\n",
    "print(f'Positive Predictive Value: {ppv_mean} (95% CI {ppv_lowerbound}-{ppv_upperbound})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hasattr(best_model,'feature_importances_'):\n",
    "    features = x_test.columns\n",
    "    importances =  best_model.feature_importances_\n",
    "    indices = np.argsort(importances)\n",
    "\n",
    "    plt.figure(figsize=(10,25))\n",
    "    plt.title('Feature Importances')\n",
    "    plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
    "    plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
    "    plt.xlabel('Relative Importance')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibrate(model,x_train,y_train,x_test,y_test,cv,plot_only):\n",
    "    \n",
    "    def plot_calibration_curve(est, name, fig_index,cv='prefit'):\n",
    "        \"\"\"\n",
    "        Ripped from: https://scikit-learn.org/stable/auto_examples/calibration/plot_compare_calibration.html#sphx-glr-auto-examples-calibration-plot-compare-calibration-py\n",
    "        \"\"\"        \n",
    "        \n",
    "        \n",
    "        \"\"\"Plot calibration curve for est w/o and with calibration. \"\"\"\n",
    "        # Calibrated with isotonic calibration\n",
    "        isotonic = CalibratedClassifierCV(est, cv=cv, method='isotonic')\n",
    "\n",
    "        # Calibrated with sigmoid calibration\n",
    "        sigmoid = CalibratedClassifierCV(est, cv=cv, method='sigmoid')\n",
    "\n",
    "        fig = plt.figure(fig_index, figsize=(10, 10))\n",
    "        ax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2)\n",
    "\n",
    "        #ax2 = plt.subplot2grid((3, 1), (2, 0))\n",
    "\n",
    "        ax1.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n",
    "        for clf, name in [(est, name),\n",
    "                          (isotonic, name + ' + Isotonic'),\n",
    "                          (sigmoid, name + ' + Sigmoid')]:\n",
    "            clf.fit(x_train, y_train)\n",
    "            y_pred = clf.predict(x_test)\n",
    "            if hasattr(clf, \"predict_proba\"):\n",
    "                prob_pos = clf.predict_proba(x_test)[:, 1]\n",
    "            else:  # use decision function\n",
    "                prob_pos = clf.decision_function(x_test)\n",
    "                prob_pos = (prob_pos - prob_pos.min()) / (prob_pos.max() - prob_pos.min())\n",
    "\n",
    "            clf_score = brier_score_loss(y_test, prob_pos, pos_label=1)\n",
    "            \n",
    "            print(\"%s:\" % name)\n",
    "            print(\"\\tBrier: %1.3f\" % (clf_score))\n",
    "            print(\"\\tPrecision: %1.3f\" % precision_score(y_test, y_pred))\n",
    "            print(\"\\tRecall: %1.3f\" % recall_score(y_test, y_pred))\n",
    "            print(\"\\tF1: %1.3f\\n\" % f1_score(y_test, y_pred))\n",
    "\n",
    "            fraction_of_positives, mean_predicted_value = \\\n",
    "                calibration_curve(y_test, prob_pos, n_bins=10)\n",
    "\n",
    "            ax1.plot(mean_predicted_value, fraction_of_positives, \"s-\",\n",
    "                     label=\"%s (%1.3f)\" % (name, clf_score))\n",
    "\n",
    "            #ax2.hist(prob_pos, range=(0, 1), bins=10, label=name,\n",
    "            #         histtype=\"step\", lw=2)\n",
    "\n",
    "        ax1.set_ylabel(\"Fraction of positives\")\n",
    "        ax1.set_ylim([-0.05, 1.05])\n",
    "        ax1.legend(loc=\"lower right\")\n",
    "        ax1.set_title('Calibration Plot')\n",
    "\n",
    "        #ax2.set_xlabel(\"Mean predicted value\")\n",
    "        #ax2.set_ylabel(\"Count\")\n",
    "        #ax2.legend(loc=\"upper center\", ncol=2)\n",
    "        plt.tight_layout()\n",
    "\n",
    "    plot_calibration_curve(model, \"Best Model\", 1)\n",
    "    \n",
    "\n",
    "calibrate(best_model,x_train,y_train,x_test,y_test,cv='prefit',plot_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mpl.rcParams['figure.dpi'] = 1200\n",
    "mpl.rcParams.update({'font.size': 11})\n",
    "\n",
    "title_auc='Figure 1A. Receiver Operating Curve'\n",
    "title_calibration='Figure 1B. Calibration Plot'\n",
    "clf = best_model\n",
    "\n",
    "y_pred_proba = clf.predict_proba(x_test.iloc[index_tpl[0]])[::,1]\n",
    "mean_fpr, mean_tpr, _ = roc_curve(y_test[index_tpl[0]],  y_pred_proba)\n",
    "\n",
    "y_pred_proba = clf.predict_proba(x_test.iloc[index_tpl[1]])[::,1]\n",
    "lower_fpr, lower_tpr, _ = roc_curve(y_test[index_tpl[1]],  y_pred_proba)\n",
    "\n",
    "y_pred_proba = clf.predict_proba(x_test.iloc[index_tpl[2]])[::,1]\n",
    "upper_fpr, upper_tpr, _ = roc_curve(y_test[index_tpl[2]],  y_pred_proba)\n",
    "\n",
    "label_string = f'AUROC: {auc_mean} (95% CI {auc_lowerbound}-{auc_upperbound})'\n",
    "\n",
    "prob_pos = clf.predict_proba(x_test)[::,1]\n",
    "clf_score = brier_score_loss(y_test, prob_pos, pos_label=1)\n",
    "\n",
    "fraction_of_positives, mean_predicted_value = calibration_curve(y_test, prob_pos, n_bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.title(title_auc)\n",
    "plt.ylim(0.,1.)\n",
    "plt.xlim(0.,1.)\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.plot([1, 0], [1, 0],\"k--\")\n",
    "plt.plot(mean_fpr,mean_tpr,label=label_string,color='red')\n",
    "plt.legend(loc=4)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title(title_calibration)\n",
    "plt.ylim(0.,1.)\n",
    "plt.xlim(0.,1.)\n",
    "plt.ylabel(\"Fraction Of Positives\")\n",
    "plt.xlabel(\"Mean Predicted Value\")\n",
    "plt.plot([1, 0], [1, 0],\"k--\")\n",
    "plt.plot(mean_predicted_value, fraction_of_positives, \"rs-\",label=f'Brier Score: {np.round(clf_score,3)}')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hasattr(best_model,'feature_importances_'):\n",
    "    features = x_test.columns\n",
    "    importances =  best_model.feature_importances_\n",
    "    indices = np.argsort(importances[:10])\n",
    "    \n",
    "    plt.figure(figsize=(5,3.2))\n",
    "    plt.title('Figure 2. Feature Importances')\n",
    "    plt.barh(range(len(indices)), importances[indices], color='r', align='center')\n",
    "    plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
    "    plt.xlabel('Relative Importance')\n",
    "    plt.xlim(0,0.05)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
